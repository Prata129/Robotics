#! /usr/bin/env python
import mdptoolbox
import numpy as np
import time

seconds = time.time()

#Transational Matrixes
#States: Up,Down,Left,Right,Stay
#[[],[],[],[],[],[],[],[],[],[]]

#UP
P0 = np.array([[0.2, 0.8, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0.2, 0.8, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0.8, 0.2, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0.2, 0.8, 0, 0, 0],
    [0 ,0 ,0, 0, 0, 0, 0.2, 0.8, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0.8, 0.2]])

#DOWN
P1 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.8, 0.2, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0.8, 0.2, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0.2, 0.8, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0.8, 0.2, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0.6, 0.4, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0.8],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])

#LEFT
P2 = np.array([[0.2, 0, 0, 0, 0.8, 0, 0, 0, 0, 0],
    [0, 0.2, 0, 0, 0.8, 0, 0, 0, 0, 0],
    [0, 0, 0.2, 0.7, 0.1, 0, 0, 0, 0, 0],
    [0, 0, 0, 0.2, 0, 0, 0.1, 0.7, 0, 0],
    [0, 0, 0, 0, 0.2, 0.4, 0.4, 0, 0, 0],
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0.2, 0.8, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])

#RIGHT
P3 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0.8, 0.2, 0, 0, 0, 0, 0, 0],
    [0.35, 0.35, 0.1, 0, 0.2, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0.8, 0.2, 0, 0, 0, 0],
    [0, 0, 0, 0.1, 0.7, 0, 0.2, 0, 0, 0],
    [0, 0, 0, 0.8, 0.2, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0.8, 0.2, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])

#STAY
P4 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])

P = np.array([P0, P1, P2, P3, P4])

#REWARDS TO USE TO ACHIEVE A GOAL
#STATES X ACTIONS
#GOAL IS STATE 0
#UP, DOWN, LEFT, RIGHT, STAY
R1 = np.array([[0, 0, -10, 0, 1],
    [0, 1, -10, 0, 0],
    [0, 0, 0, 0, 0],
    [0, -10, 0, 0, 0],
    [0, 0, 0, 0, -10],
    [0, 0, 0, -10, 0],
    [0, 0, 0, -10, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0]])


#GOAL IS STATE 9
R2 = np.array([[0, 0, -10, 0, 0],
    [0, 0, -10, 0, 0],
    [0, 0, 0, 0, 0],
    [0, -10, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, -10, 0],
    [0, 0, 0, -10, 0],
    [0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 0, 0, 0, 1]])

#VALUE ITERARION RECEIVES TRANSITION MATRIX, REWARDS, DISCOUNT FACTOR AND MAX ITERATIONS)
#VERBOSE WILL ALLOW US TO SEE RUNNING INFORMATION SUCH AS NUMBER OF ITERATIONS PASSING BY
valueIte1 = mdptoolbox.mdp.ValueIteration(P, R1, 0.90, max_iter = 10000)
valueIte1.verbose = True

valueIte2 = mdptoolbox.mdp.ValueIteration(P, R2, 0.90, max_iter = 10000)
valueIte2.verbose = True

valueIte1.run()
valueIte1.run()

print(valueIte1.policy)
print(valueIte2.policy)

print(time.time() - seconds)
